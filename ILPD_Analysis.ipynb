{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indian Liver Patient Dataset (ILPD) Analysis\n",
    "## Machine Learning and Data Analytics Coursework\n",
    "\n",
    "**Dataset**: Indian Liver Patient Dataset (ILPD)\n",
    "\n",
    "**Objective**: Predict liver disease in patients using machine learning classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, roc_auc_score,\n",
    "    precision_recall_curve\n",
    ")\n",
    "\n",
    "# Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Handling imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "The Indian Liver Patient Dataset (ILPD) contains records of 584 patients.\n",
    "\n",
    "**Features:**\n",
    "1. Age - Age of the patient\n",
    "2. Gender - Gender of the patient\n",
    "3. Total Bilirubin (TB) - mg/dL\n",
    "4. Direct Bilirubin (DB) - mg/dL\n",
    "5. Alkaline Phosphotase - IU/L\n",
    "6. Alamine Aminotransferase (ALT/SGPT) - IU/L\n",
    "7. Aspartate Aminotransferase (AST/SGOT) - IU/L\n",
    "8. Total Proteins (TP) - g/dL\n",
    "9. Albumin (ALB) - g/dL\n",
    "10. Albumin and Globulin Ratio (A/G Ratio)\n",
    "11. Target - 1 (liver patient) or 2 (non-liver patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "column_names = [\n",
    "    'Age', 'Gender', 'Total_Bilirubin', 'Direct_Bilirubin',\n",
    "    'Alkaline_Phosphotase', 'Alamine_Aminotransferase',\n",
    "    'Aspartate_Aminotransferase', 'Total_Proteins', 'Albumin',\n",
    "    'Albumin_Globulin_Ratio', 'Target'\n",
    "]\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Indian Liver Patient Dataset (ILPD).csv', names=column_names)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Number of observations: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 3.1 Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\" * 80)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 80)\n",
    "missing = df.isnull().sum()\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")\n",
    "print(f\"Percentage of duplicates: {(duplicates/len(df))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Statistical Summary\n",
    "\n",
    "Statistical summary provides measures of central tendency (mean, median) and dispersion (standard deviation, variance) for all numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VARIANCE AND STANDARD DEVIATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "stats_df = pd.DataFrame({\n",
    "    'Mean': df[numeric_cols].mean(),\n",
    "    'Median': df[numeric_cols].median(),\n",
    "    'Variance': df[numeric_cols].var(),\n",
    "    'Std Dev': df[numeric_cols].std(),\n",
    "    'Skewness': df[numeric_cols].skew(),\n",
    "    'Kurtosis': df[numeric_cols].kurtosis()\n",
    "})\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Target Variable Distribution\n",
    "\n",
    "Analyzing the distribution of the target variable to check for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TARGET VARIABLE DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "target_counts = df['Target'].value_counts()\n",
    "target_percent = df['Target'].value_counts(normalize=True) * 100\n",
    "\n",
    "target_df = pd.DataFrame({\n",
    "    'Count': target_counts,\n",
    "    'Percentage': target_percent\n",
    "})\n",
    "print(target_df)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "target_counts.plot(kind='bar', ax=axes[0], color=['#3498db', '#e74c3c'])\n",
    "axes[0].set_title('Target Variable Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Target Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Liver Patient (1)', 'Non-Liver Patient (2)'], rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(target_counts, labels=['Liver Patient (1)', 'Non-Liver Patient (2)'],\n",
    "            autopct='%1.1f%%', colors=['#3498db', '#e74c3c'], startangle=90)\n",
    "axes[1].set_title('Target Variable Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender distribution\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENDER DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gender_counts = df['Gender'].value_counts()\n",
    "print(gender_counts)\n",
    "print(f\"\\nMale percentage: {(gender_counts['Male']/len(df))*100:.2f}%\")\n",
    "print(f\"Female percentage: {(gender_counts['Female']/len(df))*100:.2f}%\")\n",
    "\n",
    "# Gender vs Target\n",
    "gender_target = pd.crosstab(df['Gender'], df['Target'], normalize='index') * 100\n",
    "print(\"\\nGender vs Target (%):\\n\", gender_target)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gender distribution\n",
    "df['Gender'].value_counts().plot(kind='bar', ax=axes[0], color=['#3498db', '#e74c3c'])\n",
    "axes[0].set_title('Gender Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Gender')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Gender vs Target\n",
    "pd.crosstab(df['Gender'], df['Target']).plot(kind='bar', ax=axes[1], color=['#3498db', '#e74c3c'])\n",
    "axes[1].set_title('Gender vs Target Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Gender')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend(['Liver Patient (1)', 'Non-Liver Patient (2)'])\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Numerical Features Distribution\n",
    "\n",
    "Analyzing the distribution of numerical features using histograms to understand data patterns and identify skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for numerical features\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.drop('Target')\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_features):\n",
    "    axes[idx].hist(df[col].dropna(), bins=30, edgecolor='black', color='#3498db', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide extra subplot\n",
    "if len(numerical_features) < len(axes):\n",
    "    for idx in range(len(numerical_features), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Box Plots - Outlier Detection\n",
    "\n",
    "Box plots help identify outliers and understand the spread of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_features):\n",
    "    axes[idx].boxplot(df[col].dropna(), vert=True, patch_artist=True,\n",
    "                      boxprops=dict(facecolor='#3498db', alpha=0.7),\n",
    "                      medianprops=dict(color='red', linewidth=2))\n",
    "    axes[idx].set_title(f'Box Plot of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel(col)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide extra subplot\n",
    "if len(numerical_features) < len(axes):\n",
    "    for idx in range(len(numerical_features), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Correlation Analysis\n",
    "\n",
    "Correlation matrix helps identify relationships between features. High correlation may indicate multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "# First encode gender for correlation\n",
    "df_temp = df.copy()\n",
    "df_temp['Gender'] = df_temp['Gender'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "correlation_matrix = df_temp.corr()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CORRELATION WITH TARGET\")\n",
    "print(\"=\" * 80)\n",
    "target_corr = correlation_matrix['Target'].sort_values(ascending=False)\n",
    "print(target_corr)\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target - bar plot\n",
    "target_corr_abs = correlation_matrix['Target'].drop('Target').abs().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "target_corr_abs.plot(kind='barh', color='#3498db')\n",
    "plt.title('Absolute Correlation with Target Variable', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Absolute Correlation')\n",
    "plt.ylabel('Features')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Feature Relationships - Scatter Plots\n",
    "\n",
    "Scatter plots show relationships between features and how they separate the target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for top correlated features\n",
    "top_features = ['Direct_Bilirubin', 'Total_Bilirubin', 'Aspartate_Aminotransferase', 'Alamine_Aminotransferase']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx in range(len(top_features)-1):\n",
    "    for class_label in df['Target'].unique():\n",
    "        subset = df[df['Target'] == class_label]\n",
    "        axes[idx].scatter(subset[top_features[idx]], subset[top_features[idx+1]],\n",
    "                         label=f'Class {class_label}', alpha=0.6, s=30)\n",
    "    \n",
    "    axes[idx].set_xlabel(top_features[idx], fontsize=11)\n",
    "    axes[idx].set_ylabel(top_features[idx+1], fontsize=11)\n",
    "    axes[idx].set_title(f'{top_features[idx]} vs {top_features[idx+1]}',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Additional scatter plot\n",
    "for class_label in df['Target'].unique():\n",
    "    subset = df[df['Target'] == class_label]\n",
    "    axes[3].scatter(subset['Age'], subset['Total_Bilirubin'],\n",
    "                   label=f'Class {class_label}', alpha=0.6, s=30)\n",
    "\n",
    "axes[3].set_xlabel('Age', fontsize=11)\n",
    "axes[3].set_ylabel('Total_Bilirubin', fontsize=11)\n",
    "axes[3].set_title('Age vs Total_Bilirubin', fontsize=12, fontweight='bold')\n",
    "axes[3].legend()\n",
    "axes[3].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Age Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution by target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "for target in df['Target'].unique():\n",
    "    subset = df[df['Target'] == target]\n",
    "    axes[0].hist(subset['Age'], bins=20, alpha=0.6, label=f'Class {target}', edgecolor='black')\n",
    "\n",
    "axes[0].set_title('Age Distribution by Target Class', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "df.boxplot(column='Age', by='Target', ax=axes[1], patch_artist=True)\n",
    "axes[1].set_title('Age Distribution by Target Class (Box Plot)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Target Class')\n",
    "axes[1].set_ylabel('Age')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAge Statistics by Target:\")\n",
    "print(df.groupby('Target')['Age'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "### 4.1 Handle Missing Values\n",
    "\n",
    "**Justification**: Missing values can negatively impact model performance. We use median imputation for the A/G Ratio as it's robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Check missing values\n",
    "print(\"Missing values before imputation:\")\n",
    "print(df_processed.isnull().sum())\n",
    "\n",
    "# Fill missing values with median (robust to outliers)\n",
    "if df_processed['Albumin_Globulin_Ratio'].isnull().sum() > 0:\n",
    "    median_ag_ratio = df_processed['Albumin_Globulin_Ratio'].median()\n",
    "    df_processed['Albumin_Globulin_Ratio'].fillna(median_ag_ratio, inplace=True)\n",
    "    print(f\"\\nFilled {df['Albumin_Globulin_Ratio'].isnull().sum()} missing values in A/G Ratio with median: {median_ag_ratio:.2f}\")\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_processed.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Handle Duplicates\n",
    "\n",
    "**Justification**: Duplicate records can lead to overfitting and biased model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "before_dup = len(df_processed)\n",
    "df_processed = df_processed.drop_duplicates()\n",
    "after_dup = len(df_processed)\n",
    "\n",
    "print(f\"Removed {before_dup - after_dup} duplicate rows\")\n",
    "print(f\"Dataset shape after removing duplicates: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Outlier Detection and Handling\n",
    "\n",
    "**Justification**: Extreme outliers can skew the model. We use IQR method to identify and cap outliers rather than removing them to preserve sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Detect outliers for each numerical column\n",
    "print(\"=\" * 80)\n",
    "print(\"OUTLIER DETECTION (IQR Method)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "outlier_summary = {}\n",
    "for col in numerical_features:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_processed, col)\n",
    "    outlier_summary[col] = {\n",
    "        'count': len(outliers),\n",
    "        'percentage': (len(outliers) / len(df_processed)) * 100,\n",
    "        'lower_bound': lower,\n",
    "        'upper_bound': upper\n",
    "    }\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Outliers: {len(outliers)} ({(len(outliers)/len(df_processed))*100:.2f}%)\")\n",
    "    print(f\"  Valid range: [{lower:.2f}, {upper:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap outliers instead of removing them (to preserve sample size)\n",
    "def cap_outliers(df, column):\n",
    "    \"\"\"Cap outliers at lower and upper bounds\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "    return df\n",
    "\n",
    "# Apply capping to extreme outliers for specific columns\n",
    "# We'll cap only the most extreme cases\n",
    "columns_to_cap = ['Total_Bilirubin', 'Direct_Bilirubin', 'Alkaline_Phosphotase',\n",
    "                  'Alamine_Aminotransferase', 'Aspartate_Aminotransferase']\n",
    "\n",
    "print(\"\\nCapping outliers for selected features...\")\n",
    "for col in columns_to_cap:\n",
    "    df_processed = cap_outliers(df_processed, col)\n",
    "    print(f\"Capped outliers for {col}\")\n",
    "\n",
    "print(\"\\nOutlier handling completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Feature Engineering\n",
    "\n",
    "**Justification**: Creating new features can capture complex relationships and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "print(\"Creating engineered features...\\n\")\n",
    "\n",
    "# 1. AST/ALT Ratio (De Ritis ratio) - important liver function indicator\n",
    "df_processed['AST_ALT_Ratio'] = (\n",
    "    df_processed['Aspartate_Aminotransferase'] / \n",
    "    (df_processed['Alamine_Aminotransferase'] + 1e-5)  # Add small value to avoid division by zero\n",
    ")\n",
    "print(\"Created: AST/ALT Ratio\")\n",
    "\n",
    "# 2. Total Protein to Albumin Ratio\n",
    "df_processed['TP_ALB_Ratio'] = (\n",
    "    df_processed['Total_Proteins'] / \n",
    "    (df_processed['Albumin'] + 1e-5)\n",
    ")\n",
    "print(\"Created: Total Protein to Albumin Ratio\")\n",
    "\n",
    "# 3. Age groups (categorical feature)\n",
    "df_processed['Age_Group'] = pd.cut(df_processed['Age'],\n",
    "                                    bins=[0, 20, 40, 60, 100],\n",
    "                                    labels=['Young', 'Adult', 'Middle_Aged', 'Senior'])\n",
    "print(\"Created: Age Groups\")\n",
    "\n",
    "# 4. Bilirubin Ratio\n",
    "df_processed['Bilirubin_Ratio'] = (\n",
    "    df_processed['Direct_Bilirubin'] / \n",
    "    (df_processed['Total_Bilirubin'] + 1e-5)\n",
    ")\n",
    "print(\"Created: Bilirubin Ratio\")\n",
    "\n",
    "# 5. Log transformation for highly skewed features\n",
    "skewed_features = ['Total_Bilirubin', 'Direct_Bilirubin', 'Alkaline_Phosphotase']\n",
    "for feature in skewed_features:\n",
    "    df_processed[f'{feature}_log'] = np.log1p(df_processed[feature])\n",
    "    print(f\"Created: {feature}_log\")\n",
    "\n",
    "print(f\"\\nDataset shape after feature engineering: {df_processed.shape}\")\n",
    "print(f\"New features added: {df_processed.shape[1] - df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Encode Categorical Variables\n",
    "\n",
    "**Justification**: Machine learning algorithms require numerical inputs. We use Label Encoding for binary categorical variable (Gender)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Gender\n",
    "le_gender = LabelEncoder()\n",
    "df_processed['Gender_Encoded'] = le_gender.fit_transform(df_processed['Gender'])\n",
    "print(\"Gender encoding:\")\n",
    "print(dict(zip(le_gender.classes_, le_gender.transform(le_gender.classes_))))\n",
    "\n",
    "# One-hot encode Age_Group\n",
    "age_group_dummies = pd.get_dummies(df_processed['Age_Group'], prefix='Age_Group')\n",
    "df_processed = pd.concat([df_processed, age_group_dummies], axis=1)\n",
    "print(\"\\nOne-hot encoded Age_Group\")\n",
    "\n",
    "# Convert target to binary (1=disease, 0=no disease)\n",
    "# Original: 1=liver patient, 2=non-liver patient\n",
    "# New: 1=liver patient, 0=non-liver patient\n",
    "df_processed['Target'] = df_processed['Target'].map({1: 1, 2: 0})\n",
    "print(\"\\nTarget variable converted to binary (1=liver disease, 0=no liver disease)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Feature Selection\n",
    "\n",
    "Select the final features for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original categorical columns and redundant features\n",
    "features_to_drop = ['Gender', 'Age_Group']\n",
    "df_model = df_processed.drop(columns=features_to_drop)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_model.drop('Target', axis=1)\n",
    "y = df_model['Target']\n",
    "\n",
    "print(f\"Final feature set shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nNumber of features: {X.shape[1]}\")\n",
    "print(f\"\\nFeature list:\")\n",
    "for idx, col in enumerate(X.columns, 1):\n",
    "    print(f\"{idx}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Train-Test Split\n",
    "\n",
    "**Justification**: Splitting data ensures unbiased evaluation. We use 80-20 split with stratification to maintain class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data split completed:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({(X_train.shape[0]/len(X))*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({(X_test.shape[0]/len(X))*100:.1f}%)\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Feature Scaling\n",
    "\n",
    "**Justification**: Many algorithms (SVM, KNN, Logistic Regression) are sensitive to feature scales. StandardScaler ensures zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for readability\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling completed using StandardScaler\")\n",
    "print(f\"\\nScaled training set statistics:\")\n",
    "print(X_train_scaled.describe().loc[['mean', 'std']].T.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Handle Class Imbalance with SMOTE\n",
    "\n",
    "**Justification**: Class imbalance can bias the model toward the majority class. SMOTE creates synthetic samples of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"Class ratio: {y_train.value_counts()[1] / y_train.value_counts()[0]:.2f}\")\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_balanced).value_counts())\n",
    "print(f\"\\nTraining samples before SMOTE: {len(X_train_scaled)}\")\n",
    "print(f\"Training samples after SMOTE: {len(X_train_balanced)}\")\n",
    "print(f\"Synthetic samples created: {len(X_train_balanced) - len(X_train_scaled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "### 5.1 Model Selection Justification\n",
    "\n",
    "We will train and compare the following algorithms:\n",
    "\n",
    "1. **Logistic Regression**: Simple, interpretable baseline model\n",
    "2. **Random Forest**: Handles non-linear relationships, robust to outliers\n",
    "3. **XGBoost**: State-of-the-art gradient boosting, handles imbalance well\n",
    "4. **Support Vector Machine (SVM)**: Effective in high-dimensional spaces\n",
    "5. **K-Nearest Neighbors**: Non-parametric, simple decision boundary\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: Minimize false positives\n",
    "- **Recall/Sensitivity (TPR)**: Minimize false negatives (critical in medical diagnosis)\n",
    "- **F1-Score**: Balance between precision and recall\n",
    "- **AUC-ROC**: Model's ability to distinguish between classes\n",
    "- **Specificity (TNR)**: True negative rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "print(\"Models initialized:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train Models and Collect Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate all metrics\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Confusion matrix elements\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['Precision'] = precision_score(y_true, y_pred, zero_division=0)\n",
    "    metrics['Recall'] = recall_score(y_true, y_pred, zero_division=0)\n",
    "    metrics['F1-Score'] = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # TPR and TNR\n",
    "    metrics['TPR (Sensitivity)'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['TNR (Specificity)'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # AUC-ROC\n",
    "    if y_pred_proba is not None:\n",
    "        metrics['AUC-ROC'] = roc_auc_score(y_true, y_pred_proba)\n",
    "    else:\n",
    "        metrics['AUC-ROC'] = 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Train models and collect results\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(y_test, y_pred, y_pred_proba)\n",
    "    results[name] = metrics\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"{name} training completed!\")\n",
    "    print(f\"  Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"  AUC-ROC: {metrics['AUC-ROC']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON - ALL METRICS\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df)\n",
    "\n",
    "# Identify best model for each metric\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST PERFORMING MODELS BY METRIC\")\n",
    "print(\"=\" * 80)\n",
    "for metric in results_df.columns:\n",
    "    best_model = results_df[metric].idxmax()\n",
    "    best_score = results_df[metric].max()\n",
    "    print(f\"{metric:20s}: {best_model:25s} ({best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "results_df['Accuracy'].plot(kind='barh', ax=axes[0, 0], color='#3498db')\n",
    "axes[0, 0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Accuracy')\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: AUC-ROC comparison\n",
    "results_df['AUC-ROC'].plot(kind='barh', ax=axes[0, 1], color='#e74c3c')\n",
    "axes[0, 1].set_title('Model AUC-ROC Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('AUC-ROC')\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 3: Precision vs Recall\n",
    "results_df[['Precision', 'Recall']].plot(kind='barh', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Precision vs Recall Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Score')\n",
    "axes[1, 0].legend(loc='best')\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 4: F1-Score comparison\n",
    "results_df['F1-Score'].plot(kind='barh', ax=axes[1, 1], color='#2ecc71')\n",
    "axes[1, 1].set_title('Model F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('F1-Score')\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, model) in enumerate(trained_models.items()):\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['No Disease', 'Disease'],\n",
    "                yticklabels=['No Disease', 'Disease'])\n",
    "    axes[idx].set_title(f'{name}\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "# Hide extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=2)\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Hyperparameter Tuning\n",
    "\n",
    "**Justification**: Fine-tuning hyperparameters can significantly improve model performance. We use GridSearchCV with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest (best performing model)\n",
    "print(\"Performing hyperparameter tuning for Random Forest...\\n\")\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"\\nBest parameters for Random Forest:\")\n",
    "print(rf_grid.best_params_)\n",
    "print(f\"\\nBest cross-validation AUC-ROC: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "rf_tuned = rf_grid.best_estimator_\n",
    "y_pred_tuned = rf_tuned.predict(X_test_scaled)\n",
    "y_pred_proba_tuned = rf_tuned.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "metrics_tuned = calculate_metrics(y_test, y_pred_tuned, y_pred_proba_tuned)\n",
    "\n",
    "print(\"\\nTuned Random Forest Performance:\")\n",
    "for metric, value in metrics_tuned.items():\n",
    "    print(f\"  {metric:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for XGBoost\n",
    "print(\"Performing hyperparameter tuning for XGBoost...\\n\")\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    param_grid_xgb,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xgb_grid.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"\\nBest parameters for XGBoost:\")\n",
    "print(xgb_grid.best_params_)\n",
    "print(f\"\\nBest cross-validation AUC-ROC: {xgb_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "xgb_tuned = xgb_grid.best_estimator_\n",
    "y_pred_xgb_tuned = xgb_tuned.predict(X_test_scaled)\n",
    "y_pred_proba_xgb_tuned = xgb_tuned.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "metrics_xgb_tuned = calculate_metrics(y_test, y_pred_xgb_tuned, y_pred_proba_xgb_tuned)\n",
    "\n",
    "print(\"\\nTuned XGBoost Performance:\")\n",
    "for metric, value in metrics_xgb_tuned.items():\n",
    "    print(f\"  {metric:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_tuned.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE (Random Forest)\")\n",
    "print(\"=\" * 80)\n",
    "print(feature_importance_rf)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance_rf['Feature'][:15], feature_importance_rf['Importance'][:15], color='#3498db')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Top 15 Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation for all models\n",
    "print(\"=\" * 80)\n",
    "print(\"CROSS-VALIDATION ANALYSIS (5-Fold)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cv_results = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    scores = cross_val_score(model, X_train_balanced, y_train_balanced,\n",
    "                            cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    cv_results[name] = {\n",
    "        'Mean AUC': scores.mean(),\n",
    "        'Std AUC': scores.std(),\n",
    "        'Min AUC': scores.min(),\n",
    "        'Max AUC': scores.max()\n",
    "    }\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results).T\n",
    "print(cv_df)\n",
    "\n",
    "# Visualize CV results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.errorbar(range(len(cv_df)), cv_df['Mean AUC'], yerr=cv_df['Std AUC'],\n",
    "             fmt='o', markersize=8, capsize=5, capthick=2, linewidth=2)\n",
    "plt.xticks(range(len(cv_df)), cv_df.index, rotation=45, ha='right')\n",
    "plt.ylabel('AUC-ROC', fontsize=12)\n",
    "plt.title('Cross-Validation Results (Mean Â± Std)', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9 Final Model Selection and Save\n",
    "\n",
    "**Final Model Selection**: Based on comprehensive evaluation, we select the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on AUC-ROC\n",
    "best_model_name = results_df['AUC-ROC'].idxmax()\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL MODEL SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSelected Model: {best_model_name}\")\n",
    "print(f\"\\nFinal Model Performance:\")\n",
    "for metric, value in results[best_model_name].items():\n",
    "    print(f\"  {metric:20s}: {value:.4f}\")\n",
    "\n",
    "# Save the best model and preprocessing objects\n",
    "joblib.dump(rf_tuned, 'models/random_forest_model.pkl')\n",
    "joblib.dump(xgb_tuned, 'models/xgboost_model.pkl')\n",
    "joblib.dump(scaler, 'models/scaler.pkl')\n",
    "joblib.dump(le_gender, 'models/label_encoder.pkl')\n",
    "\n",
    "print(\"\\nModels and preprocessing objects saved successfully!\")\n",
    "print(\"  - models/random_forest_model.pkl\")\n",
    "print(\"  - models/xgboost_model.pkl\")\n",
    "print(\"  - models/scaler.pkl\")\n",
    "print(\"  - models/label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Interpretation and Insights\n",
    "\n",
    "### 6.1 Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"=\" * 80)\n",
    "print(\"CLASSIFICATION REPORT - FINAL MODEL\")\n",
    "print(\"=\" * 80)\n",
    "y_pred_final = rf_tuned.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred_final,\n",
    "                          target_names=['No Disease', 'Liver Disease']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the entire analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset: Indian Liver Patient Dataset (ILPD)\")\n",
    "print(f\"Total Samples: {len(df)}\")\n",
    "print(f\"Total Features (Original): {df.shape[1] - 1}\")\n",
    "print(f\"Total Features (After Engineering): {X.shape[1]}\")\n",
    "print(f\"Training Samples: {len(X_train)} ({len(X_train_balanced)} after SMOTE)\")\n",
    "print(f\"Test Samples: {len(X_test)}\")\n",
    "print(f\"\\nModels Evaluated: {len(models)}\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best AUC-ROC: {results[best_model_name]['AUC-ROC']:.4f}\")\n",
    "print(f\"Best Accuracy: {results[best_model_name]['Accuracy']:.4f}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This analysis successfully developed and evaluated multiple machine learning models for predicting liver disease. The key findings include:\n",
    "\n",
    "1. **Data Quality**: The dataset contained minimal missing values and duplicates were handled appropriately.\n",
    "\n",
    "2. **Feature Engineering**: Created meaningful features including AST/ALT ratio, bilirubin ratio, and log transformations that improved model performance.\n",
    "\n",
    "3. **Class Imbalance**: Addressed using SMOTE, which improved minority class detection.\n",
    "\n",
    "4. **Model Performance**: Random Forest and XGBoost demonstrated superior performance with AUC-ROC scores above 0.70.\n",
    "\n",
    "5. **Key Predictors**: Direct Bilirubin, Total Bilirubin, and liver enzyme ratios were the most important features.\n",
    "\n",
    "**Limitations**:\n",
    "- Limited dataset size (584 samples)\n",
    "- Potential selection bias in data collection\n",
    "- Missing clinical context for some extreme values\n",
    "\n",
    "**Future Work**:\n",
    "- Collect more diverse patient data\n",
    "- Incorporate additional clinical features\n",
    "- Explore deep learning approaches\n",
    "- Develop ensemble methods combining multiple models\n",
    "\n",
    "**Ethical Considerations**:\n",
    "- Model should be used as a decision support tool, not replacement for medical professionals\n",
    "- Regular monitoring and validation required for clinical deployment\n",
    "- Patient privacy and data security must be maintained\n",
    "- Potential bias in predictions should be continuously evaluated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
